{"cumulative_reward": -0.2, "epsilon": 1.0, "episode": 0, "agent_steps": 1, "simulation_steps": 1.0, "Distance": 0, "_timestamp": 1717469498.8942, "_runtime": 74.91638517379761, "_step": 0}